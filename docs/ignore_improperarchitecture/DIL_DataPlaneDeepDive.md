Ancillary data flows — where to store what, what to capture, and how to turn it into training & governance
Great question — this is the gnarly but extremely important plumbing that determines whether your platform is reproducible, auditable, learnable, and safe. Below I’ll walk you through:the data categories you should capture from agents and platform flows,where to store them (recommended services/collections),the minimal fields to capture for each agent call (minimum viable data),how to structure agent config files for reproducibility and governance,how to capture training data safely and turn it into retrainable datasets,how to store WAL / Saga data and use it for replay/compensation,retention / security / de-id policies,a short set of cursor-friendly tasks your team can implement quickly.
I’ll be concrete — you can hand the schemas and tasks to engineers.1 — High level: data categories & ownershipAgent runtime metadata (Agentic Foundation) — owner: Platform (Agentic Foundation)What: prompt_template_id, prompt_hash, prompt_version, toolset invoked, pre/post-hook info, redaction flags, model_version, latency, cost_estUse: auditing, billing, debugging, explainabilityAgent I/O artifacts (Inputs/Outputs) — owner: Tenant (raw) / Platform (metadata)What: raw prompt/input (tenant-only), model response (tenant-only or de-identified copy if allowed), structured outputs (e.g., mapping candidates)Use: UI review, replay, training (after de-id/approval)Agent config artifacts (Curator / Librarian) — owner: Platform + Team ownersWhat: agent JSON config (persona, tool bindings, pre/post hooks, validation tests, canary policy), prompt templatesUse: reproducibility, versioning, governanceEmbeddings & semantic artifacts (DIF + Arango) — owner: DIF/tenantWhat: vectors, confidence, model_meta, artifact_refsUse: retrieval, matching, analyticsMapping history / human approvals (Arango / DIF audit) — owner: Tenant + PlatformWhat: decisions, reviewer_id, timestamps, justificationUse: training labels, auditWAL & Saga state (DIL-Orchestration in DIF) — owner: DIF orchestratorWhat: append-only steps, intended compensation, saga_id, step stateUse: replay, compensation, idempotencyObservability / Telemetry (Nurse / DIF) — owner: PlatformWhat: traces, metrics, errors, costsUse: SLOs, alerts, optimizationTraining dataset (Training catalog in DIF / tenant-controlled storage) — owner: Platform (global) & tenant (local)What: de-identified examples, features, labels, provenanceUse: model retrain, active learning2 — Recommended storage map (concrete)Raw tenant inputs & raw model responses: Tenant S3 bucket (encrypted), tenant-only access. (StorageAdapter)Agent config files & prompt templates: Curator (git-like store + version IDs). (Curator Foundation)Prompt metadata / prompt events (no raw prompt): DIF prompt_events collection in Arango (store prompt_hash, template_id, model_version, redaction_applied, trace_id).Agent I/O artifacts for audit/training (de-identified): DIF training bucket (parquet) + training catalog references in DIF.Embeddings & semantic docs: Arango collections: embeddings_* (tenant-tagged) + vector index (ArangoSearch or VectorStoreAdapter).Mapping history / approvals: Arango mapping_history collection (immutable entries).WAL / Saga: Arango tenant_wal collection (append-only) and saga_state collection for current state; optionally Kafka for high-throughput WAL streams.Observability: OTEL traces (Jaeger/Honeycomb), metrics (Prometheus/ClickHouse), logs (ELK/Cloud logging) — store pointers & aggregates in DIF.Model artifacts: Model registry entries in DIF (model_registry) with model artifact refs in Storage (S3).Training datasets: Storage artifacts (parquet) with catalog metadata inside DIF (training_catalog collection).3 — Minimum viable data to capture for each agent call
Capture this minimal set for every LLM/agent invocation. It’s small, cheap, and essential.trace_id (global)saga_id (if part of a workflow)tenant_idagent_id (e.g., mapping.agent.v1)agent_config_id and agent_config_version (Curator reference)prompt_template_id and prompt_version (Librarian reference)prompt_hash (sha256 of the rendered prompt AFTER redaction) — do not store raw prompt unless tenant permitsredaction_applied (bool) — whether pre-hook redaction ran and succeededinput_artifact_refs (list of artifact ids, pointers, not raw input)model_id, model_version, endpointstart_ts, end_ts, latency_mscost_estimate (USD cents)response_summary: short structured summary (JSON) produced by post-hook — e.g., {candidates_count: 3, top_candidate: "sem.customer.name"}response_artifact_ref — pointer to structured outputs stored in tenant storage (JSON) or Arangosuccess boolean and error_code if anymetrics: e.g., token_count_in/out, bytes_sentexecution_env (realm, node id)parent_trace_id (if chained)
Where to put it: DIF prompt_events collection (Arango) + OTEL trace with same trace_id.4 — Agent config JSON structure (recommended schema)
Store these in Curator as versioned artifacts. Minimal viable fields:{ "agent_id": "mapping.agent.v1", "version": "2025-12-01", "description": "Mapping agent for column -> semantic matching", "owner_team": "data-mesh", "persona": { "role": "data-mapper", "instructions": "You are an expert data mapper. Use tables and samples to suggest semantic ids." }, "prompt_template_id": "pt.mapping.v3", "tools": [ { "name": "dif.semantic_match", "interface_id": "dif.match.v1", "timeout_ms": 15000, "retries": 1 }, { "name": "arango.upsert_mapping", "interface_id": "arango.mapping.upsert.v1", "required": true } ], "pre_hooks": [ { "name": "redact", "policy_ref": "tenant_default" }, { "name": "summarize", "params": { "max_sentences": 3 } } ], "post_hooks": [ { "name": "extract_candidates" }, { "name": "normalize_confidences" } ], "validation": { "unit_tests": [ { "name": "return_json_only", "spec": "json_schema id..." } ] }, "canary": { "enabled": true, "percent": 5 }}Why this format?Links to prompt templates and tools (no inline prompt text unless tenant-approved).Encodes pre/post-hooks for consistent redaction and structured outputs.Enables programmatic validation and canarying.5 — Capturing & using training data (safe pipeline)
Design constraintsNever use raw PII in global training datasets unless tenant explicitly allows and keys/policies are in place.Always prefer de-identified examples or synthetic augmentation.Record provenance for each training example (artifact_ref, mapping_history_id, trainer_version).
Flow to capture training examplesUser approves a mapping -> mapping_history entry written (Arango) with reviewer_id and timestamp.Delivery Orchestrator / DIL process consumes mapping_history entries and builds a training example:input_features: embedding vector OR engineered features (name_similarity, dtype, sample_overlap)label: semantic_idmeta: prompt_hash, model_version, trace_id, mapping_history_idDe-identify input:Strip raw samples from features, or replace with hashed/synth valuesOption: store original sample in tenant-only bucket with a secure pointer (not included in training data)Write example to training bucket (parquet) and register it in DIF training_catalog with allowed fields flagged.Periodic retrain (Delivery Orchestrator triggers retrain job): reads approved training dataset refs from DIF, trains model in isolated environment, stores new model artifact, registers model in DIF model_registry, emits model-version events (Curator policy required to promote to production).Canary / Shadow evaluation: before promoting, run model on shadow traffic and compute acceptance/override rates.
Minimal training example schema{ "example_id": "ex-20251208-001", "tenant_scope": "global|tenantX", "features": { "col_name_tokens": [...], "embedding_vector_ref": "s3://.../vec_123.parquet#row=12", "name_similarity": 0.83, "dtype_score": 1, "sample_coverage": 0.67, "regex_match": 1 }, "label": "sem.customer.name", "meta": { "mapping_history_id": "mh_123", "trace_id": "tr_abc", "agent_config_id": "mapping.agent.v1", "model_version": "v1", "approved_by": "user_joe" }, "created_at": "2025-12-08T12:00:00Z"}Where to store training artifacts: storage bucket (parquet) referenced by DIF training_catalog.6 — WAL & Saga storage and patterns (more detail)
WAL (append-only)Use Arango tenant_wal with schema:wal_id, saga_id, tenant_id, step_name, payload_ref, compensate_ref, status (pending|done|failed), created_at, completed_atWAL entries are append-only. Use DB access control rules to prevent deletes (soft delete only).
Saga statesaga_state collection:saga_id, current_step, status, owner_agent_id, retries, last_update_tsSaga engine reads WAL entries and orchestrates compensation steps as needed.
Usage tipsAppend WAL before the external action (write-ahead). If external action fails, saga uses compensate_ref to run rollback.Keep idempotency keys in WAL entries so steps can be retried safely.WAL entries should include trace_id for correlation.
Storage considerationsWAL size and retention depend on recovery needs. Keep 90–365 days by default and archive to cold storage for longer regulatory retention.7 — Observability, telemetry & correlation
Inject trace_id everywhereAt the UI start, create trace_id.Propagate trace_id to all agents, WAL entries, prompt events, embeddings, mapping_history entries, and training examples.
OTEL spansEach agent call = OTEL span with attributes:agent_id, agent_config_id, prompt_hash, model_id, cost_est, tenant_id, saga_idUse a distributed tracing backend; correlate to logs and metrics.
Metrics to collectper-agent: calls/sec, avg_latency_ms, p50/p95/p99, error_rateper-model: tokens_in, tokens_out, cost_est_total, inference_secondsper-tenant: model_costs, embedding_count, storage_sizeper-semantic_id: acceptance_rate, override_rate8 — Security, retention, and compliance recommendations (non-negotiables)Encryption at rest & in transit for all storage (S3 + Arango + DBs)Tenant isolation of raw inputs and raw responsesDIF stores only metadata + de-identified training data by defaultAudit trail: mapping_history + prompt_events + wal entries are append-only and immutableRBAC: Curator-managed roles: PlatformAdmin, TenantAdmin, Steward, Auditor, ReviewerLegal hold: support legal holds on tenant artifacts via DIF flaggingRetention policies: per-category (raw client inputs may have long retention; prompt metadata shorter)9 — Short implementation checklist (cursor-friendly tasks)DIF-DB-001 Create Arango collections: prompt_events, mapping_history, embeddings, training_catalog, tenant_wal, saga_state. AC: insert and query sample docs.CUR-001 Implement Curator agent_config storage API. AC: save versioned config and fetch by id+version.AGENTSDK-001 Implement prompt_hash logging in Agentic SDK with call to POST /dif/prompt_events. AC: DIF receives prompt_event with required minimal fields.HF-AGENT-001 Build stateless HF agent skeleton that returns embedding_vector, confidence, model_meta, and writes response_artifact_ref to tenant storage. AC: embedding stored and pointer returned.WAL-001 Implement POST /dif/wal/append API and write sample WAL entries pre/post embedding. AC: WAL entries are append-only and queryable by saga_id.TRAIN-PIPE-001 Implement job that converts mapping_history accepted rows into de-identified training examples and writes Parquet to training bucket and registers them in training_catalog. AC: training dataset exists and is referenced in DIF.OBS-001 Add OTEL instrumentation to agent SDK and Orchestrator with attributes above. AC: trace shows prompt_event → embedding → mapping → approval.POLICY-001 Implement tenant redaction policy store and run sample agent call with redaction_pre_hook. AC: prompt_hash recorded, raw prompt not stored.10 — Final operational notes & pitfallsDon’t over-collect raw data — collect the minimal set needed for audit & replay; prefer pointers to tenant-owned raw data.Hash rather than store raw prompts whenever possible (and keep a tenant-only copy if they want replay).Version everything — agent configs, prompt templates, model versions. This is how you trace behavior changes.Make training ingestion auditable — every training example should have mapping_history_id and trace_id.Use canaries & shadow runs to test new models before full promotion.Plan for scale: embeddings can balloon; store vectors in efficient stores (Arango vector fields or vector DB) and implement cleanup of superseded vectors to control costs.