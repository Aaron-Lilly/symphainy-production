#!/usr/bin/env python3
"""
Test Guide Agent Pattern - GuideCrossDomainAgent

Tests the guide agent pattern: cross-domain navigation, liaison routing,
and user journey tracking with conversation history.

Tests:
1. Agent initialization
2. YAML config loading
3. Solution configuration (MVP domains)
4. LLM integration (real API calls)
5. Cross-domain intent understanding
6. Conversation history maintenance
7. Cost tracking
8. Stateful behavior verification

Uses PRODUCTION environment to ensure fixes actually work.
"""

import sys
import os
from pathlib import Path

# Configure for PRODUCTION environment
# Use production endpoints instead of localhost
PRODUCTION_BASE_URL = os.getenv("PRODUCTION_BASE_URL", "http://35.215.64.103")
# Traefik API might be on port 80 (via Traefik routing) or 8080 (direct API access)
# Try port 80 first (standard Traefik routing), fallback to 8080
PRODUCTION_TRAEFIK_URL = os.getenv("TRAEFIK_API_URL", f"{PRODUCTION_BASE_URL}:80")
PRODUCTION_MCP_URL = os.getenv("MCP_SERVER_URL", f"{PRODUCTION_BASE_URL}:8000/mcp")

# Set production environment variables
# Use production URLs but keep ENVIRONMENT as "development" to avoid strict production checks
os.environ["TRAEFIK_API_URL"] = PRODUCTION_TRAEFIK_URL
os.environ["MCP_SERVER_URL"] = PRODUCTION_MCP_URL
os.environ["ENVIRONMENT"] = "development"  # Keep as development to avoid OTEL requirements
os.environ["TEST_MODE"] = "true"  # Mark as test mode
os.environ["TRAEFIK_OPTIONAL_IN_TEST"] = "true"  # Make Traefik optional for this test

print(f"ðŸ”§ Using PRODUCTION environment:")
print(f"   Base URL: {PRODUCTION_BASE_URL}")
print(f"   Traefik: {PRODUCTION_TRAEFIK_URL}")
print(f"   MCP Server: {PRODUCTION_MCP_URL}")
print()

# Cost control configuration (using TestConfig)
os.environ.setdefault("TEST_USE_REAL_LLM", "false") # Set to true to test with real LLM
os.environ.setdefault("TEST_USE_CHEAPEST_MODEL", "true")
os.environ.setdefault("TEST_ENABLE_RETRIES", "false")
os.environ.setdefault("TEST_MAX_TOKENS", "100") # Allow more tokens for meaningful responses
os.environ.setdefault("TEST_TRACK_COSTS", "true")
os.environ.setdefault("TEST_MAX_COST", "1.00") # Max cost for guide agent test
os.environ.setdefault("TEST_USE_CACHE", "true")

# Add project root to path (match stateless test pattern exactly)
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "symphainy-platform"))
sys.path.insert(0, str(project_root / "tests"))

import asyncio
from typing import Dict, Any, Optional
from datetime import datetime
import json
import logging

# Import cost management utilities
from tests.test_config import TestConfig
from tests.fixtures.llm_response_cache import LLMResponseCache
from tests.utils.cost_tracker import get_cost_tracker

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("GuideAgentTest")

# Initialize cost tracking and response cache
cost_tracker = get_cost_tracker(max_cost=TestConfig.get_max_cost())
response_cache = LLMResponseCache() if TestConfig.USE_RESPONSE_CACHE else None

# Test results tracking
test_results = {
    "total": 0,
    "passed": 0,
    "failed": 0,
    "errors": []
}

def log_test(test_name: str, status: str, error: Optional[str] = None, details: Optional[Dict[str, Any]] = None):
    """Log test result."""
    test_results["total"] += 1
    if status == "PASS":
        test_results["passed"] += 1
        logger.info(f"âœ… {test_name}")
        if details:
            for key, value in details.items():
                logger.info(f"   {key}: {value}")
    else:
        test_results["failed"] += 1
        error_msg = f"{test_name}: {error}" if error else test_name
        test_results["errors"].append(error_msg)
        logger.error(f"âŒ {test_name}: {error}")
        if details:
            for key, value in details.items():
                logger.error(f"   {key}: {value}")

# ============================================================================
# TEST SETUP
# ============================================================================

async def setup_platform_services():
    """Initialize platform services."""
    logger.info("ðŸ”§ Setting up platform services...")
    
    try:
        from foundations.di_container.di_container_service import DIContainerService
        from foundations.public_works_foundation.public_works_foundation_service import PublicWorksFoundationService
        from foundations.curator_foundation.curator_foundation_service import CuratorFoundationService
        from foundations.agentic_foundation.agentic_foundation_service import AgenticFoundationService
        
        # Initialize DI Container
        di_container = DIContainerService("test_guide_agent")
        
        # Initialize Public Works Foundation
        public_works = PublicWorksFoundationService(di_container)
        await public_works.initialize_foundation()
        di_container.service_registry["PublicWorksFoundationService"] = public_works
        
        # Verify initialization
        if not public_works.is_initialized:
            raise RuntimeError("Public Works Foundation initialization failed - is_initialized is False")
        
        # Initialize Curator Foundation
        curator = CuratorFoundationService(
            foundation_services=di_container,
            public_works_foundation=public_works
        )
        await curator.initialize()
        di_container.service_registry["CuratorFoundationService"] = curator
        
        # Initialize Agentic Foundation
        agentic_foundation = AgenticFoundationService(
            di_container=di_container,
            public_works_foundation=public_works,
            curator_foundation=curator
        )
        await agentic_foundation.initialize()
        di_container.service_registry["AgenticFoundationService"] = agentic_foundation
        
        logger.info("âœ… Platform services initialized")
        
        return {
            "di_container": di_container,
            "public_works": public_works,
            "curator": curator,
            "agentic_foundation": agentic_foundation
        }
        
    except Exception as e:
        logger.error(f"âŒ Platform setup failed: {e}")
        import traceback
        traceback.print_exc()
        raise

async def setup_agent(platform_services: Dict[str, Any]):
    """Initialize GuideCrossDomainAgent (declarative)."""
    logger.info("ðŸ”§ Setting up GuideCrossDomainAgent (declarative)...")
    
    try:
        from backend.business_enablement.agents.guide_cross_domain_agent import GuideCrossDomainAgent
        from foundations.agentic_foundation.agent_sdk.policy_integration import PolicyIntegration
        from foundations.agentic_foundation.agent_sdk.tool_composition import ToolComposition
        
        agentic_foundation = platform_services["agentic_foundation"]
        di_container = platform_services["di_container"]
        public_works = platform_services["public_works"]
        curator = platform_services["curator"]
        
        # Instantiate SDK components
        policy_integration = PolicyIntegration(
            foundation_services=di_container,
            agentic_foundation=agentic_foundation
        )
        tool_composition = ToolComposition(
            foundation_services=di_container,
            agentic_foundation=agentic_foundation
        )
        agui_formatter = agentic_foundation.agui_formatter
        mcp_client_manager = agentic_foundation.mcp_client_manager
        
        # Initialize agent (using declarative version)
        agent = GuideCrossDomainAgent(
            agent_name="MVP Guide Agent",
            foundation_services=di_container,
            agentic_foundation=agentic_foundation,
            mcp_client_manager=mcp_client_manager,
            policy_integration=policy_integration,
            tool_composition=tool_composition,
            agui_formatter=agui_formatter,
            curator_foundation=curator,
            metadata_foundation=None,
            public_works_foundation=public_works,
            logger=None
        )
        
        await agent.initialize()
        
        logger.info("âœ… GuideCrossDomainAgent initialized")
        return agent
        
    except Exception as e:
        logger.error(f"âŒ Agent setup failed: {e}")
        import traceback
        traceback.print_exc()
        raise

# ============================================================================
# TEST 1: Agent Initialization and Config Verification
# ============================================================================

async def test_agent_initialization_and_config(agent):
    """Test agent initializes correctly and loads config."""
    test_name = "Agent Initialization and Config Verification"
    try:
        assert agent is not None, "Agent should be initialized"
        assert agent.agent_name == "MVPGuideAgent", "Agent name mismatch"
        assert agent.stateful is True, "Agent should be stateful"
        assert agent.max_conversation_history > 0, "Agent should have conversation history limit"
        assert agent.iterative_execution is False, "Agent should be single-pass (conversational)"
        assert agent.cost_tracking is True, "Cost tracking should be enabled"
        assert hasattr(agent, 'configured_domains'), "Agent should have configured_domains"
        assert len(agent.configured_domains) > 0, "Agent should have at least one domain configured"
        
        log_test(test_name, "PASS", details={
            "Agent Name": agent.agent_name,
            "Stateful": agent.stateful,
            "Max Conversation History": agent.max_conversation_history,
            "Iterative Execution": agent.iterative_execution,
            "Cost Tracking": agent.cost_tracking,
            "Configured Domains": agent.configured_domains,
            "Solution Type": agent.solution_type
        })
        return True
    except AssertionError as e:
        log_test(test_name, "FAIL", str(e))
        return False
    except Exception as e:
        log_test(test_name, "ERROR", str(e))
        return False

# ============================================================================
# TEST 2: Simple Cross-Domain Request
# ============================================================================

async def test_simple_cross_domain_request(agent):
    """Test handling a simple cross-domain request."""
    test_name = "Simple Cross-Domain Request"
    try:
        user_context = {"user_id": "test_user_1", "tenant_id": "test_tenant_1", "role": "manager"}
        
        request = {
            "message": "I need help navigating the MVP solution. What can you help me with?",
            "user_context": user_context
        }
        
        logger.info("   ðŸ§ª Making a simple cross-domain request...")
        
        result = await agent.handle_user_request(request)
        
        logger.info(f"   Response: {json.dumps(result, indent=2)}")
        
        assert result.get("type") == "guide_response", "Response should be guide_response type"
        assert "message" in result, "Response should contain 'message'"
        assert "cost_info" in result, "Response should include cost_info"
        assert result["cost_info"]["total_cost"] > 0, "Cost should be greater than 0"
        assert "configured_domains" in result, "Response should include configured_domains"
        
        log_test(test_name, "PASS", details={
            "Response Type": result.get("type"),
            "Message Length": len(result.get("message", "")),
            "Total Cost": result["cost_info"]["total_cost"],
            "Configured Domains": result.get("configured_domains", [])
        })
        return True
    except AssertionError as e:
        log_test(test_name, "FAIL", str(e))
        return False
    except Exception as e:
        log_test(test_name, "ERROR", str(e))
        import traceback
        traceback.print_exc()
        return False

# ============================================================================
# TEST 3: Cross-Domain Intent Understanding
# ============================================================================

async def test_cross_domain_intent(agent):
    """Test understanding cross-domain intent."""
    test_name = "Cross-Domain Intent Understanding"
    try:
        user_context = {"user_id": "test_user_2", "tenant_id": "test_tenant_2"}
        
        # Request that spans multiple domains
        request = {
            "message": "I want to analyze content performance and then create operational reports.",
            "user_context": user_context
        }
        
        logger.info("   ðŸ§ª Making cross-domain request...")
        result = await agent.handle_user_request(request)
        
        # Verify agent understands cross-domain intent
        assert result.get("type") == "guide_response", "Response should be guide_response type"
        assert "message" in result, "Response should contain 'message'"
        # The agent should recognize this spans multiple domains (content_management, insights_analysis, operations_management)
        
        log_test(test_name, "PASS", details={
            "Response Type": result.get("type"),
            "Intent": result.get("intent", "general"),
            "Suggested Routes": len(result.get("suggested_routes", []))
        })
        return True
    except AssertionError as e:
        log_test(test_name, "FAIL", str(e))
        return False
    except Exception as e:
        log_test(test_name, "ERROR", str(e))
        import traceback
        traceback.print_exc()
        return False

# ============================================================================
# TEST 4: Stateful Behavior (Conversation History)
# ============================================================================

async def test_stateful_behavior(agent):
    """Verify that the agent maintains conversation history."""
    test_name = "Stateful Behavior Verification"
    try:
        user_context = {"user_id": "test_user_3", "tenant_id": "test_tenant_3"}
        
        # First request
        request_1 = {
            "message": "What domains are available in the MVP solution?",
            "user_context": user_context
        }
        
        logger.info("   ðŸ§ª Making first request...")
        result_1 = await agent.handle_user_request(request_1)
        
        # Check conversation history length
        history_length_1 = result_1.get("conversation_history_length", 0)
        assert history_length_1 > 0, "Stateful agent should maintain conversation history"
        
        # Second request (should have context from first)
        request_2 = {
            "message": "Can you tell me more about the content management domain?",
            "user_context": user_context
        }
        
        logger.info("   ðŸ§ª Making second request (should have context)...")
        result_2 = await agent.handle_user_request(request_2)
        
        history_length_2 = result_2.get("conversation_history_length", 0)
        assert history_length_2 > history_length_1, "Conversation history should grow"
        
        # Verify agent has conversation history
        assert hasattr(agent, 'conversation_history'), "Agent should have conversation_history attribute"
        assert len(agent.conversation_history) > 0, "Conversation history should not be empty"
        
        log_test(test_name, "PASS", details={
            "First Request History Length": history_length_1,
            "Second Request History Length": history_length_2,
            "Agent History Length": len(agent.conversation_history)
        })
        return True
    except AssertionError as e:
        log_test(test_name, "FAIL", str(e))
        return False
    except Exception as e:
        log_test(test_name, "ERROR", str(e))
        import traceback
        traceback.print_exc()
        return False

# ============================================================================
# TEST 5: Cost Tracking
# ============================================================================

async def test_cost_tracking(agent):
    """Test cost tracking across multiple requests."""
    test_name = "Cost Tracking"
    try:
        user_context = {"user_id": "test_user_4", "tenant_id": "test_tenant_4"}
        
        # Make a request
        request = {
            "message": "Help me understand the MVP solution capabilities.",
            "user_context": user_context
        }
        
        result = await agent.handle_user_request(request)
        
        # Verify cost info
        assert "cost_info" in result, "Response should include cost_info"
        cost_info = result["cost_info"]
        assert "total_cost" in cost_info, "Cost info should include total_cost"
        assert cost_info["total_cost"] > 0, "Cost should be greater than 0"
        
        # Verify agent's internal cost tracking
        assert hasattr(agent, '_total_cost'), "Agent should have _total_cost attribute"
        assert agent._total_cost > 0, "Agent's total cost should be greater than 0"
        
        log_test(test_name, "PASS", details={
            "Total Cost": cost_info["total_cost"],
            "Agent Internal Cost": agent._total_cost,
            "Total Operations": cost_info.get("total_operations", 0)
        })
        return True
    except AssertionError as e:
        log_test(test_name, "FAIL", str(e))
        return False
    except Exception as e:
        log_test(test_name, "ERROR", str(e))
        import traceback
        traceback.print_exc()
        return False

# ============================================================================
# MAIN TEST EXECUTION
# ============================================================================

async def main():
    logger.info("================================================================================")
    logger.info("TEST: Guide Agent Pattern - GuideCrossDomainAgent")
    logger.info("================================================================================")
    
    logger.info(f"\nðŸ“‹ Test Configuration:")
    logger.info(f"   Environment: PRODUCTION")
    logger.info(f"   USE_REAL_LLM: {TestConfig.USE_REAL_LLM}")
    logger.info(f"   USE_CHEAPEST_MODEL: {TestConfig.USE_CHEAPEST_MODEL}")
    logger.info(f"   MAX_TEST_COST: ${TestConfig.MAX_TEST_COST}")
    logger.info(f"   USE_CACHE: {TestConfig.USE_RESPONSE_CACHE}")
    logger.info(f"   Traefik URL: {os.getenv('TRAEFIK_API_URL')}")
    logger.info(f"   MCP Server URL: {os.getenv('MCP_SERVER_URL')}")
    logger.info("\n")
    
    platform_services = None
    agent = None
    
    try:
        platform_services = await setup_platform_services()
        agent = await setup_agent(platform_services)
        
        # Run tests
        await test_agent_initialization_and_config(agent)
        await test_simple_cross_domain_request(agent)
        await test_cross_domain_intent(agent)
        await test_stateful_behavior(agent)
        await test_cost_tracking(agent)
        
    except Exception as e:
        logger.critical(f"Fatal error during test execution: {e}")
        test_results["errors"].append(f"Fatal error: {e}")
        test_results["failed"] = test_results["total"] - test_results["passed"]
    finally:
        logger.info("\n================================================================================")
        logger.info("TEST SUMMARY")
        logger.info("================================================================================")
        logger.info(f"Total Tests: {test_results['total']}")
        logger.info(f"Passed: {test_results['passed']}")
        logger.info(f"Failed: {test_results['failed']}")
        if test_results["errors"]:
            logger.info("Errors:")
            for error in test_results["errors"]:
                logger.error(f"  - {error}")
        
        cost_summary = cost_tracker.get_summary()
        logger.info(f"ðŸ’° Cost Summary:")
        logger.info(f"   Total Cost: ${cost_summary['total_cost']:.4f}")
        logger.info(f"   Total Calls: {cost_summary['total_calls']}")
        logger.info(f"   Remaining Budget: ${cost_summary['remaining_budget']:.4f}")
        
        # Cleanup (if needed)
        if platform_services:
            logger.info("âœ… Test completed.")
        
        if test_results["failed"] > 0:
            sys.exit(1) # Indicate failure

if __name__ == "__main__":
    asyncio.run(main())

